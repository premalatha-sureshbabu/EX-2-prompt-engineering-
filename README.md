# EX-2-prompt-engineering-Comparative Analysis of different types of Prompting patterns and explain with Various Test scenerios

## Experiment:
Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
     Analyze the quality, accuracy, and depth of the generated responses.

## Experiment Objective:
To test and compare how different prompting patterns—broad/unstructured vs. basic/refined—affect the performance of language models in generating responses. The focus is on assessing the quality, accuracy, and depth of responses across varied scenarios.

### Prompting Patterns Overview:

Broad/Unstructured Prompts: Open-ended and general; lack specific direction.

Basic/Refined Prompts: Clear, concise, and goal-oriented; contain sufficient detail to guide the response.

### Test Scenarios and Comparative Output:

### Scenario 1: General Knowledge

Broad: "Tell me about AI."

Refined: "Explain how AI is applied in medical diagnostics."

Result:

Broad prompt leads to a high-level overview lacking context.

Refined prompt yields a detailed, application-specific explanation.

### Scenario 2: Creative Writing

Broad: "Write a story."

Refined: "Write a science fiction story involving a time machine and ethical dilemmas."

Result:

Broad prompt produces an aimless or generic narrative.

Refined prompt produces a structured, engaging, and creative story.

### Scenario 3: Coding Task

Broad: "Write Python code."

Refined: "Write a Python function that checks if a number is prime."

Result:

Broad prompt outputs unrelated or trivial code.

Refined prompt gives concise, accurate, and directly useful code.

### Scenario 4: Opinion-Based Response

Broad: "What do you think of education?"

Refined: "What are the pros and cons of remote learning in universities?"

Result:

Broad prompt leads to vague generalities.

Refined prompt gives a balanced and evidence-based analysis.

### Scenario 5: Troubleshooting

Broad: "Fix my code."

Refined: "Why does my Python code throw a TypeError when summing a list of integers?"

Result:

Broad prompt results in unclear or generic responses.

Refined prompt gives a specific solution and explanation.

# OUTPUT
![image](https://github.com/user-attachments/assets/578e1bd6-cfbd-48ae-9ba5-af4657237e2a)


# RESULT
Through structured testing across multiple scenarios, refined (basic) prompts consistently outperformed broad (unstructured) prompts in terms of:

    Quality – more coherent and context-specific outputs

    Accuracy – fewer misunderstandings and better relevance

    Depth – richer, more detailed, and meaningful responses

Refined prompts led to precise, actionable content, especially in technical, coding, and analytical tasks. Broad prompts, while flexible, often resulted in vague or overly generic responses. This demonstrates the importance of clear intent and specificity when interacting with language models.
